# Datasets info
cmu:
  dataset_dir: ""
  val_dataset_dir: "/home/xuli/researchfiler/occdata/CMU/"
  train_img_txt: ""
  val_img_txt: "/home/xuli/researchfiler/occdata/CMU/cmu_val30.txt"
  
piod:
  dataset_dir: "/home/xuli/researchfiler/occdata/PIOD/"
  val_dataset_dir: "/home/xuli/researchfiler/occdata/PIOD/"
  train_img_txt: "/home/xuli/researchfiler/occdata/PIOD/train_ids.lst"
  val_img_txt: "/home/xuli/researchfiler/occdata/PIOD/val_doc_2010.txt"

bsds:
  dataset_dir: "/home/xuli/researchfiler/occdata/BSDSownership/"
  val_dataset_dir: "/home/xuli/researchfiler/occdata/BSDSownership/"
  train_img_txt: "/home/xuli/researchfiler/occdata/BSDSownership/train.lst"
  val_img_txt: "/home/xuli/researchfiler/occdata/BSDSownership/test.lst"
  
nyuocpp:
  dataset_dir: "/home/xuli/researchfiler/occdata/NYUv2-OCpp/"
  val_dataset_dir: "/home/xuli/researchfiler/occdata/NYUv2-OCpp/"
  train_img_txt: "/home/xuli/researchfiler/occdata/NYUv2-OCpp/train.txt"
  val_img_txt: "/home/xuli/researchfiler/occdata/NYUv2-OCpp/test.txt"
  
# ob-future
synocc:
  dataset_dir: "/home/xuli/researchfiler/occdata/synocc/"
  val_dataset_dir: "/home/xuli/researchfiler/occdata/synocc/"
  train_img_txt: "/home/xuli/researchfiler/occdata/synocc_split/synocc_train_3.txt"
  val_img_txt: "/home/xuli/researchfiler/occdata/synocc_split/synocc_fval.txt"

# ob-future rgba  
synocc_rgba:
  dataset_dir: "/home/xuli/researchfiler/occdata/synocc/"
  val_dataset_dir: "/home/xuli/researchfiler/occdata/synocc/"
  train_img_txt: "/home/xuli/researchfiler/occdata/synocc_split/synocc_train_3.txt"
  val_img_txt: "/home/xuli/researchfiler/occdata/synocc_split/synocc_fval.txt"

# ob-diode
diode:
  dataset_dir: ""
  val_dataset_dir: "/home/xuli/researchfiler/occdata/DIODE/"
  train_img_txt: ""
  val_img_txt: "/home/xuli/researchfiler/occdata/DIODE/diode_val50.txt"

# ob-entityseg
entityseg:
  dataset_dir: ""
  val_dataset_dir: "/home/xuli/researchfiler/occdata/EntitySeg/"
  train_img_txt: ""
  val_img_txt: "/home/xuli/researchfiler/occdata/EntitySeg/entityseg_val70.txt"


# You can download the weights for HRNet from the repository:
# https://github.com/HRNet/HRNet-Image-Classification
PRETRAINED_MODELS:
  # hrnetv2
  hrnet18s: "../pretrained/hrnet_w18_small_model_v2.pth"
  hrnet18: "../pretrained/hrnetv2_w18_imagenet_pretrained.pth"
  hrnet32: "../pretrained/hrnetv2_w32_imagenet_pretrained.pth"
  hrnet64: "../pretrained/hrnetv2_w64_imagenet_pretrained.pth"
  resnet34: "../pretrained/resnet34-333f7ec4.pth"
  # resnet50: "../pretrained/resnet50s-a75c83cf.pth"
  # resnet101: "../pretrained/resnet101s-03a0f310.pth"  
  resnet50: "../pretrained/resnet50s-a75c83cf.pth"
  resnet101: "../pretrained/resnet101s-03a0f310.pth"  
  gluon_resnet34: "../pretrained/gluon_resnet34_v1b-c6d82d59.pth"
  resnest50: ""    
  segformer_b3: "../pretrained/mit_b3.pth"
  segformer_b0: "../pretrained/mit_b0.pth"
  hrformer_base: "../pretrained/hrt_base.pth"
  swinformer_small: "../pretrained/swin_small_patch4_window7_224.pth"  
  swinformer_base: "../pretrained/swin_base_patch4_window12_384_22k.pth"
  swinformer_large: "../pretrained/swin_large_patch4_window12_384_22k.pth"
  plainvit_base: "../pretrained/mae_pretrain_vit_base.pth"
  plainvit_large: "../pretrained/mae_pretrain_vit_large.pth"
  plainvit_huge: "../pretrained/mae_pretrain_vit_huge.pth"

